{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"(Anu) 2-handWritingGenTrain.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### importing required data"],"metadata":{"id":"mQwyBUcQeivD"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"ZAxj-Tb6E8mp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641197869675,"user_tz":-330,"elapsed":36987,"user":{"displayName":"Anu Sree","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7pQ0bmhnHiKFrTiED7uuMcCb0zd-5OOYmVT9-dg=s64","userId":"05137986881356860231"}},"outputId":"f1f76ccf-0603-4760-938d-c166d1f7a098"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# %rm -rf data\n","# %rm -rf logstrain_scribe.txt\n","# %rm -rf model.py\n","# %rm -rf utils.py"],"metadata":{"id":"ACNWzqijg_nx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!cp -r \"/content/model\" \"/content/drive/MyDrive/Colab Notebooks/Handwriting Generation/model\""],"metadata":{"id":"DSCirGADY81n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641124821492,"user_tz":-330,"elapsed":894,"user":{"displayName":"preeti","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05611806873912134402"}},"outputId":"448f933f-490d-43ad-8323-accccc09456c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cp: cannot stat '/content/model': No such file or directory\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ev4Hgbe2mtXq"},"outputs":[],"source":["!mkdir '/content/data'\n","!cp -r \"/content/drive/MyDrive/Colab Notebooks/Handwriting Generation/data/ascii-all.tar.gz\" \"/content/data/ascii-all.tar.gz\"\n","!cp -r \"/content/drive/MyDrive/Colab Notebooks/Handwriting Generation/data/lineStrokes-all.tar.gz\" \"/content/data/lineStrokes-all.tar.gz\""]},{"cell_type":"code","source":["!cp \"/content/drive/MyDrive/Colab Notebooks/Handwriting Generation/model.py\" \"model.py\""],"metadata":{"id":"AJF4uaduApK7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!cp \"/content/sample.py\" \"/content/drive/MyDrive/Colab Notebooks/Handwriting Generation/sample.py\" \n","!cp \"/content/drive/MyDrive/Colab Notebooks/Handwriting Generation/sample.py\" \"/content/sample.py\""],"metadata":{"id":"-_ichePsh4zE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp \"/content/drive/MyDrive/Colab Notebooks/Handwriting Generation/utils.py\" \"utils.py\""],"metadata":{"id":"StmBy0b4YctC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp \"/content/drive/MyDrive/Colab Notebooks/Handwriting Generation/data/strokes_training_data.cpkl\" \"data/strokes_training_data.cpkl\""],"metadata":{"id":"-SfhPW79RS1x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!cp -r \"/content/drive/MyDrive/Colab Notebooks/Handwriting Generation/model\" \"/content/model\"\n","!cp -r \"/content/drive/MyDrive/Colab Notebooks/Handwriting Generation/model\" \"/content/model1\""],"metadata":{"id":"Gl95TCyxg5Ew"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!tar -xf data/ascii-all.tar.gz\n","!mv ascii data/ascii"],"metadata":{"id":"KwsWg4Ge6U__"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!tar -xf data/lineStrokes-all.tar.gz\n","!mv lineStrokes data/lineStrokes"],"metadata":{"id":"mOWsrf-D6YhO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install svgwrite"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8jdf7idrE_uP","executionInfo":{"status":"ok","timestamp":1641197987214,"user_tz":-330,"elapsed":4542,"user":{"displayName":"Anu Sree","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7pQ0bmhnHiKFrTiED7uuMcCb0zd-5OOYmVT9-dg=s64","userId":"05137986881356860231"}},"outputId":"c72c5594-ef5b-4341-aa32-640d8f821d8e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting svgwrite\n","  Downloading svgwrite-1.4.1-py3-none-any.whl (66 kB)\n","\u001b[?25l\r\u001b[K     |█████                           | 10 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 20 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 30 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 40 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 66 kB 2.9 MB/s \n","\u001b[?25hInstalling collected packages: svgwrite\n","Successfully installed svgwrite-1.4.1\n"]}]},{"cell_type":"markdown","source":["# imports\n"],"metadata":{"id":"QbuM3Evxi4ny"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wXQiHu013Stb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641197998073,"user_tz":-330,"elapsed":3639,"user":{"displayName":"Anu Sree","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7pQ0bmhnHiKFrTiED7uuMcCb0zd-5OOYmVT9-dg=s64","userId":"05137986881356860231"}},"outputId":"a5fd03cb-9be7-411f-e529-3f835e24acce"},"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow 1.x selected.\n"]}],"source":["import numpy as np\n","import numpy.matlib\n","import math\n","import random\n","import os\n","import xml.etree.ElementTree as ET\n","import importlib\n","import pickle\n","\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","import time\n","from utils import *"]},{"cell_type":"markdown","source":["# data loading"],"metadata":{"id":"8OwxTKdSi9Qn"}},{"cell_type":"code","source":["\n","class DataLoader():\n","    def __init__(self, args, logger, limit = 500):\n","        self.data_dir = args.data_dir\n","        self.alphabet = args.alphabet\n","        self.batch_size = args.batch_size\n","        self.tsteps = args.tsteps\n","        self.data_scale = args.data_scale # scale data down by this factor\n","        self.ascii_steps = args.tsteps/args.tsteps_per_ascii\n","        self.logger = logger\n","        self.limit = limit # removes large noisy gaps in the data\n","\n","        data_file = os.path.join(self.data_dir, \"strokes_training_data.cpkl\")\n","        stroke_dir = self.data_dir + \"/lineStrokes\"\n","        ascii_dir = self.data_dir + \"/ascii\"\n","\n","        if not (os.path.exists(data_file)) :\n","            self.logger.write(\"\\tcreating training data cpkl file from raw source\")\n","            self.preprocess(stroke_dir, ascii_dir, data_file)\n","\n","        self.load_preprocessed(data_file)\n","        self.reset_batch_pointer()\n","\n","    def preprocess(self, stroke_dir, ascii_dir, data_file):\n","        # create data file from raw xml files from iam handwriting source.\n","        self.logger.write(\"\\tparsing dataset...\")\n","        \n","        # build the list of xml files\n","        filelist = []\n","        # Set the directory you want to start from\n","        rootDir = stroke_dir\n","        for dirName, subdirList, fileList in os.walk(rootDir):\n","            for fname in fileList:\n","                filelist.append(dirName+\"/\"+fname)\n","\n","        # function to read each individual xml file\n","        def getStrokes(filename):\n","            tree = ET.parse(filename)\n","            root = tree.getroot()\n","\n","            result = []\n","\n","            x_offset = 1e20\n","            y_offset = 1e20\n","            y_height = 0\n","            for i in range(1, 4):\n","                x_offset = min(x_offset, float(root[0][i].attrib['x']))\n","                y_offset = min(y_offset, float(root[0][i].attrib['y']))\n","                y_height = max(y_height, float(root[0][i].attrib['y']))\n","            y_height -= y_offset\n","            x_offset -= 100\n","            y_offset -= 100\n","\n","            for stroke in root[1].findall('Stroke'):\n","                points = []\n","                for point in stroke.findall('Point'):\n","                    points.append([float(point.attrib['x'])-x_offset,float(point.attrib['y'])-y_offset])\n","                result.append(points)\n","            return result\n","        \n","        # function to read each individual xml file\n","        def getAscii(filename, line_number):\n","            with open(filename, \"r\") as f:\n","                s = f.read()\n","            s = s[s.find(\"CSR\"):]\n","            if len(s.split(\"\\n\")) > line_number+2:\n","                s = s.split(\"\\n\")[line_number+2]\n","                return s\n","            else:\n","                return \"\"\n","                \n","        # converts a list of arrays into a 2d numpy int16 array\n","        def convert_stroke_to_array(stroke):\n","            n_point = 0\n","            for i in range(len(stroke)):\n","                n_point += len(stroke[i])\n","            stroke_data = np.zeros((n_point, 3), dtype=np.int16)\n","\n","            prev_x = 0\n","            prev_y = 0\n","            counter = 0\n","\n","            for j in range(len(stroke)):\n","                for k in range(len(stroke[j])):\n","                    stroke_data[counter, 0] = int(stroke[j][k][0]) - prev_x\n","                    stroke_data[counter, 1] = int(stroke[j][k][1]) - prev_y\n","                    prev_x = int(stroke[j][k][0])\n","                    prev_y = int(stroke[j][k][1])\n","                    stroke_data[counter, 2] = 0\n","                    if (k == (len(stroke[j])-1)): \n","                        stroke_data[counter, 2] = 1\n","                    counter += 1\n","            return stroke_data\n","\n","        # build stroke database of every xml file inside iam database\n","        strokes = []\n","        asciis = []\n","        for i in range(len(filelist)):\n","            if (filelist[i][-3:] == 'xml'):\n","                stroke_file = filelist[i]\n","#                 print 'processing '+stroke_file\n","                stroke = convert_stroke_to_array(getStrokes(stroke_file))\n","                \n","                ascii_file = stroke_file.replace(\"lineStrokes\",\"ascii\")[:-7] + \".txt\"\n","                line_number = stroke_file[-6:-4]\n","                line_number = int(line_number) - 1\n","                ascii = getAscii(ascii_file, line_number)\n","                if len(ascii) > 10:\n","                    strokes.append(stroke)\n","                    asciis.append(ascii)\n","                else:\n","                    self.logger.write(\"\\tline length was too short. line was: \" + ascii)\n","                \n","        assert(len(strokes)==len(asciis)), \"There should be a 1:1 correspondence between stroke data and ascii labels.\"\n","        f = open(data_file,\"wb\")\n","        pickle.dump([strokes,asciis], f, protocol=2)\n","        f.close()\n","        self.logger.write(\"\\tfinished parsing dataset. saved {} lines\".format(len(strokes)))\n","\n","\n","    def load_preprocessed(self, data_file):\n","        f = open(data_file,\"rb\")\n","        [self.raw_stroke_data, self.raw_ascii_data] = pickle.load(f)\n","        f.close()\n","\n","        # goes thru the list, and only keeps the text entries that have more than tsteps points\n","        self.stroke_data = []\n","        self.ascii_data = []\n","        self.valid_stroke_data = []\n","        self.valid_ascii_data = []\n","        counter = 0\n","\n","        # every 1 in 20 (5%) will be used for validation data\n","        cur_data_counter = 0\n","        for i in range(len(self.raw_stroke_data)):\n","            data = self.raw_stroke_data[i]\n","            if len(data) > (self.tsteps+2):\n","                # removes large gaps from the data\n","                data = np.minimum(data, self.limit)\n","                data = np.maximum(data, -self.limit)\n","                data = np.array(data,dtype=np.float32)\n","                data[:,0:2] /= self.data_scale\n","                cur_data_counter = cur_data_counter + 1\n","                if cur_data_counter % 20 == 0:\n","                  self.valid_stroke_data.append(data)\n","                  self.valid_ascii_data.append(self.raw_ascii_data[i])\n","                else:\n","                    self.stroke_data.append(data)\n","                    self.ascii_data.append(self.raw_ascii_data[i])\n","\n","        # minus 1, since we want the ydata to be a shifted version of x data\n","        self.num_batches = int(len(self.stroke_data) / self.batch_size)\n","        self.logger.write(\"\\tloaded dataset:\")\n","        self.logger.write(\"\\t\\t{} train individual data points\".format(len(self.stroke_data)))\n","        self.logger.write(\"\\t\\t{} valid individual data points\".format(len(self.valid_stroke_data)))\n","        self.logger.write(\"\\t\\t{} batches\".format(self.num_batches))\n","\n","    def validation_data(self):\n","        # returns validation data\n","        x_batch = []\n","        y_batch = []\n","        ascii_list = []\n","        for i in range(self.batch_size):\n","            valid_ix = i%len(self.valid_stroke_data)\n","            data = self.valid_stroke_data[valid_ix]\n","            x_batch.append(np.copy(data[:self.tsteps]))\n","            y_batch.append(np.copy(data[1:self.tsteps+1]))\n","            ascii_list.append(self.valid_ascii_data[valid_ix])\n","        one_hots = [to_one_hot(s, self.ascii_steps, self.alphabet) for s in ascii_list]\n","        return x_batch, y_batch, ascii_list, one_hots\n","\n","    def next_batch(self):\n","        # returns a randomized, tsteps-sized portion of the training data\n","        x_batch = []\n","        y_batch = []\n","        ascii_list = []\n","        for i in range(self.batch_size):\n","            data = self.stroke_data[self.idx_perm[self.pointer]]\n","            idx = random.randint(0, len(data)-self.tsteps-2)\n","            x_batch.append(np.copy(data[:self.tsteps]))\n","            y_batch.append(np.copy(data[1:self.tsteps+1]))\n","            ascii_list.append(self.ascii_data[self.idx_perm[self.pointer]])\n","            self.tick_batch_pointer()\n","        one_hots = [to_one_hot(s, self.ascii_steps, self.alphabet) for s in ascii_list]\n","        return x_batch, y_batch, ascii_list, one_hots\n","\n","    def tick_batch_pointer(self):\n","        self.pointer += 1\n","        if (self.pointer >= len(self.stroke_data)):\n","            self.reset_batch_pointer()\n","    def reset_batch_pointer(self):\n","        self.idx_perm = np.random.permutation(len(self.stroke_data))\n","        self.pointer = 0"],"metadata":{"id":"Mpt8CoquCmVu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# utility function for converting input ascii characters into vectors the network can understand.\n","# index position 0 means \"unknown\"\n","def to_one_hot(s, ascii_steps, alphabet):\n","    steplimit=3e3; s = s[:3e3] if len(s) > 3e3 else s # clip super-long strings\n","    seq = [alphabet.find(char) + 1 for char in s]\n","    ascii_steps = int(ascii_steps)\n","    if len(seq) >= ascii_steps:\n","      seq = seq[: ascii_steps]\n","    else:\n","      seq = seq + [0]*(ascii_steps - len(seq))\n","    one_hot = np.zeros((ascii_steps,len(alphabet)+1))\n","    one_hot[np.arange(ascii_steps),seq] = 1\n","    return one_hot\n"],"metadata":{"id":"CcochJFGDAdp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# model definitions"],"metadata":{"id":"2NAxSbuYjA3P"}},{"cell_type":"code","source":["class Model():\n","\tdef __init__(self, args, logger):\n","\t\tself.logger = logger\n","\n","\t\t# ----- transfer some of the args params over to the model\n","\n","\t\t# model params\n","\t\tself.rnn_size = args.rnn_size\n","\t\tself.train = args.train\n","\t\tself.nmixtures = args.nmixtures\n","\t\tself.kmixtures = args.kmixtures\n","\t\tself.batch_size = args.batch_size if self.train else 1 # training/sampling specific\n","\t\tself.tsteps = args.tsteps if self.train else 1 # training/sampling specific\n","\t\tself.alphabet = args.alphabet\n","\t\t# training params\n","\t\tself.dropout = args.dropout\n","\t\tself.grad_clip = args.grad_clip\n","\t\t# misc\n","\t\tself.tsteps_per_ascii = args.tsteps_per_ascii\n","\t\tself.data_dir = args.data_dir\n","\n","\t\tself.graves_initializer = tf.truncated_normal_initializer(mean=0., stddev=.075, seed=None, dtype=tf.float32)\n","\t\tself.window_b_initializer = tf.truncated_normal_initializer(mean=-3.0, stddev=.25, seed=None, dtype=tf.float32) # hacky initialization\n","\n","\t\tself.logger.write('\\tusing alphabet{}'.format(self.alphabet))\n","\t\tself.char_vec_len = len(self.alphabet) + 1 #plus one for <UNK> token\n","\t\tself.ascii_steps = args.tsteps/args.tsteps_per_ascii\n","\n","\n","\t\t# ----- build the basic recurrent network architecture\n","\t\tcell_func = tf.contrib.rnn.LSTMCell # could be GRUCell or RNNCell\n","\t\tself.cell0 = cell_func(args.rnn_size, state_is_tuple=True, initializer=self.graves_initializer)\n","\t\tself.cell1 = cell_func(args.rnn_size, state_is_tuple=True, initializer=self.graves_initializer)\n","\t\tself.cell2 = cell_func(args.rnn_size, state_is_tuple=True, initializer=self.graves_initializer)\n","\n","\t\tif (self.train and self.dropout < 1): # training mode\n","\t\t\tself.cell0 = tf.contrib.rnn.DropoutWrapper(self.cell0, output_keep_prob = self.dropout)\n","\t\t\tself.cell1 = tf.contrib.rnn.DropoutWrapper(self.cell1, output_keep_prob = self.dropout)\n","\t\t\tself.cell2 = tf.contrib.rnn.DropoutWrapper(self.cell2, output_keep_prob = self.dropout)\n","\n","\t\tself.input_data = tf.placeholder(dtype=tf.float32, shape=[None, self.tsteps, 3])\n","\t\tself.target_data = tf.placeholder(dtype=tf.float32, shape=[None, self.tsteps, 3])\n","\t\tself.istate_cell0 = self.cell0.zero_state(batch_size=self.batch_size, dtype=tf.float32)\n","\t\tself.istate_cell1 = self.cell1.zero_state(batch_size=self.batch_size, dtype=tf.float32)\n","\t\tself.istate_cell2 = self.cell2.zero_state(batch_size=self.batch_size, dtype=tf.float32)\n","\n","\t\t#slice the input volume into separate vols for each tstep\n","\t\tinputs = [tf.squeeze(input_, [1]) for input_ in tf.split(self.input_data, self.tsteps, 1)]\n","\t\t#build cell0 computational graph\n","\t\touts_cell0, self.fstate_cell0 = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, self.istate_cell0, self.cell0, loop_function=None, scope='cell0')\n","\n","\n","\t# ----- build the gaussian character window\n","\t\tdef get_window(alpha, beta, kappa, c):\n","\t\t\t# phi -> [? x 1 x ascii_steps] and is a tf matrix\n","\t\t\t# c -> [? x ascii_steps x alphabet] and is a tf matrix\n","\t\t\tascii_steps = c.get_shape()[1].value #number of items in sequence\n","\t\t\tphi = get_phi(ascii_steps, alpha, beta, kappa)\n","\t\t\twindow = tf.matmul(phi,c)\n","\t\t\twindow = tf.squeeze(window, [1]) # window ~ [?,alphabet]\n","\t\t\treturn window, phi\n","\n","\t\t#get phi for all t,u (returns a [1 x tsteps] matrix) that defines the window\n","\t\tdef get_phi(ascii_steps, alpha, beta, kappa):\n","\t\t\t# alpha, beta, kappa -> [?,kmixtures,1] and each is a tf variable\n","\t\t\tu = np.linspace(0,ascii_steps-1,ascii_steps) # weight all the U items in the sequence\n","\t\t\tkappa_term = tf.square( tf.subtract(kappa,u))\n","\t\t\texp_term = tf.multiply(-beta,kappa_term)\n","\t\t\tphi_k = tf.multiply(alpha, tf.exp(exp_term))\n","\t\t\tphi = tf.reduce_sum(phi_k,1, keep_dims=True)\n","\t\t\treturn phi # phi ~ [?,1,ascii_steps]\n","\n","\t\tdef get_window_params(i, out_cell0, kmixtures, prev_kappa, reuse=True):\n","\t\t\thidden = out_cell0.get_shape()[1]\n","\t\t\tn_out = 3*kmixtures\n","\t\t\twith tf.variable_scope('window',reuse=reuse):\n","\t\t\t\twindow_w = tf.get_variable(\"window_w\", [hidden, n_out], initializer=self.graves_initializer)\n","\t\t\t\twindow_b = tf.get_variable(\"window_b\", [n_out], initializer=self.window_b_initializer)\n","\t\t\tabk_hats = tf.nn.xw_plus_b(out_cell0, window_w, window_b) # abk_hats ~ [?,n_out]\n","\t\t\tabk = tf.exp(tf.reshape(abk_hats, [-1, 3*kmixtures,1])) # abk_hats ~ [?,n_out] = \"alpha, beta, kappa hats\"\n","\n","\t\t\talpha, beta, kappa = tf.split(abk, 3, 1) # alpha_hat, etc ~ [?,kmixtures]\n","\t\t\tkappa = kappa + prev_kappa\n","\t\t\treturn alpha, beta, kappa # each ~ [?,kmixtures,1]\n","\n","\t\tself.init_kappa = tf.placeholder(dtype=tf.float32, shape=[None, self.kmixtures, 1]) \n","\t\tself.char_seq = tf.placeholder(dtype=tf.float32, shape=[None, self.ascii_steps, self.char_vec_len])\n","\t\tprev_kappa = self.init_kappa\n","\t\tprev_window = self.char_seq[:,0,:]\n","\n","\t\t#add gaussian window result\n","\t\treuse = False\n","\t\tfor i in range(len(outs_cell0)):\n","\t\t\t[alpha, beta, new_kappa] = get_window_params(i, outs_cell0[i], self.kmixtures, prev_kappa, reuse=reuse)\n","\t\t\twindow, phi = get_window(alpha, beta, new_kappa, self.char_seq)\n","\t\t\touts_cell0[i] = tf.concat((outs_cell0[i],window), 1) #concat outputs\n","\t\t\touts_cell0[i] = tf.concat((outs_cell0[i],inputs[i]), 1) #concat input data\n","\t\t\tprev_kappa = new_kappa\n","\t\t\tprev_window = window\n","\t\t\treuse = True\n","\t\t#save some attention mechanism params (useful for sampling/debugging later)\n","\t\tself.window = window\n","\t\tself.phi = phi\n","\t\tself.new_kappa = new_kappa\n","\t\tself.alpha = alpha\n","\n","\n","\t# ----- finish building LSTMs 2 and 3\n","\t\touts_cell1, self.fstate_cell1 = tf.contrib.legacy_seq2seq.rnn_decoder(outs_cell0, self.istate_cell1, self.cell1, loop_function=None, scope='cell1')\n","\n","\t\touts_cell2, self.fstate_cell2 = tf.contrib.legacy_seq2seq.rnn_decoder(outs_cell1, self.istate_cell2, self.cell2, loop_function=None, scope='cell2')\n","\n","\t# ----- start building the Mixture Density Network on top (start with a dense layer to predict the MDN params)\n","\t\tn_out = 1 + self.nmixtures * 6 # params = end_of_stroke + 6 parameters per Gaussian\n","\t\twith tf.variable_scope('mdn_dense'):\n","\t\t\tmdn_w = tf.get_variable(\"output_w\", [self.rnn_size, n_out], initializer=self.graves_initializer)\n","\t\t\tmdn_b = tf.get_variable(\"output_b\", [n_out], initializer=self.graves_initializer)\n","\n","\t\tout_cell2 = tf.reshape(tf.concat(outs_cell2, 1), [-1, args.rnn_size]) #concat outputs for efficiency\n","\t\toutput = tf.nn.xw_plus_b(out_cell2, mdn_w, mdn_b) #data flows through dense nn\n","\n","\n","\t# ----- build mixture density cap on top of second recurrent cell\n","\t\tdef gaussian2d(x1, x2, mu1, mu2, s1, s2, rho):\n","\t\t\t# define gaussian mdn (eq 24, 25 from http://arxiv.org/abs/1308.0850)\n","\t\t\tx_mu1 = tf.subtract(x1, mu1)\n","\t\t\tx_mu2 = tf.subtract(x2, mu2)\n","\t\t\tZ = tf.square(tf.div(x_mu1, s1)) + \\\n","\t\t\t    tf.square(tf.div(x_mu2, s2)) - \\\n","\t\t\t    2*tf.div(tf.multiply(rho, tf.multiply(x_mu1, x_mu2)), tf.multiply(s1, s2))\n","\t\t\trho_square_term = 1-tf.square(rho)\n","\t\t\tpower_e = tf.exp(tf.div(-Z,2*rho_square_term))\n","\t\t\tregularize_term = 2*np.pi*tf.multiply(tf.multiply(s1, s2), tf.sqrt(rho_square_term))\n","\t\t\tgaussian = tf.div(power_e, regularize_term)\n","\t\t\treturn gaussian\n","\n","\t\tdef get_loss(pi, x1_data, x2_data, eos_data, mu1, mu2, sigma1, sigma2, rho, eos):\n","\t\t\t# define loss function (eq 26 of http://arxiv.org/abs/1308.0850)\n","\t\t\tgaussian = gaussian2d(x1_data, x2_data, mu1, mu2, sigma1, sigma2, rho)\n","\t\t\tterm1 = tf.multiply(gaussian, pi)\n","\t\t\tterm1 = tf.reduce_sum(term1, 1, keep_dims=True) #do inner summation\n","\t\t\tterm1 = -tf.log(tf.maximum(term1, 1e-20)) # some errors are zero -> numerical errors.\n","\n","\t\t\tterm2 = tf.multiply(eos, eos_data) + tf.multiply(1-eos, 1-eos_data) #modified Bernoulli -> eos probability\n","\t\t\tterm2 = -tf.log(term2) #negative log error gives loss\n","\n","\t\t\treturn tf.reduce_sum(term1 + term2) #do outer summation\n","\n","\t\t# now transform dense NN outputs into params for MDN\n","\t\tdef get_mdn_coef(Z):\n","\t\t\t# returns the tf slices containing mdn dist params (eq 18...23 of http://arxiv.org/abs/1308.0850)\n","\t\t\teos_hat = Z[:, 0:1] #end of sentence tokens\n","\t\t\tpi_hat, mu1_hat, mu2_hat, sigma1_hat, sigma2_hat, rho_hat = tf.split(Z[:, 1:], 6, 1)\n","\t\t\tself.pi_hat, self.sigma1_hat, self.sigma2_hat = \\\n","\t\t\t\t\t\t\t\t\t\tpi_hat, sigma1_hat, sigma2_hat # these are useful for bias method during sampling\n","\n","\t\t\teos = tf.sigmoid(-1*eos_hat) # technically we gained a negative sign\n","\t\t\tpi = tf.nn.softmax(pi_hat) # softmax z_pi:\n","\t\t\tmu1 = mu1_hat; mu2 = mu2_hat # leave mu1, mu2 as they are\n","\t\t\tsigma1 = tf.exp(sigma1_hat); sigma2 = tf.exp(sigma2_hat) # exp for sigmas\n","\t\t\trho = tf.tanh(rho_hat) # tanh for rho (squish between -1 and 1)rrr\n","\n","\t\t\treturn [eos, pi, mu1, mu2, sigma1, sigma2, rho]\n","\n","\t\t# reshape target data (as we did the input data)\n","\t\tflat_target_data = tf.reshape(self.target_data,[-1, 3])\n","\t\t[x1_data, x2_data, eos_data] = tf.split(flat_target_data, 3, 1) #we might as well split these now\n","\n","\t\t[self.eos, self.pi, self.mu1, self.mu2, self.sigma1, self.sigma2, self.rho] = get_mdn_coef(output)\n","\n","\t\tloss = get_loss(self.pi, x1_data, x2_data, eos_data, self.mu1, self.mu2, self.sigma1, self.sigma2, self.rho, self.eos)\n","\t\tself.cost = loss / (self.batch_size * self.tsteps)\n","\n","\t\t# ----- bring together all variables and prepare for training\n","\t\tself.learning_rate = tf.Variable(0.0, trainable=False)\n","\t\tself.decay = tf.Variable(0.0, trainable=False)\n","\t\tself.momentum = tf.Variable(0.0, trainable=False)\n","\n","\t\ttvars = tf.trainable_variables()\n","\t\tgrads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars), self.grad_clip)\n","\n","\t\tif args.optimizer == 'adam':\n","\t\t\tself.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n","\t\telif args.optimizer == 'rmsprop':\n","\t\t\tself.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate, decay=self.decay, momentum=self.momentum)\n","\t\telse:\n","\t\t\traise ValueError(\"Optimizer type not recognized\")\n","\t\tself.train_op = self.optimizer.apply_gradients(zip(grads, tvars))\n","\n","\t\t# ----- some TensorFlow I/O\n","\t\tself.sess = tf.InteractiveSession()\n","\t\tself.saver = tf.train.Saver(tf.global_variables())\n","\t\tself.sess.run(tf.global_variables_initializer())\n","\n","\t\t# ----- for restoring previous models\n","\tdef try_load_model(self, save_path):\n","\t\tload_was_success = True # yes, I'm being optimistic\n","\t\tglobal_step = 0\n","\t\ttry:\n","\t\t\tsave_dir = '/'.join(save_path.split('/')[:-1])\n","\t\t\tckpt = tf.train.get_checkpoint_state(save_dir)\n","\t\t\tload_path = ckpt.model_checkpoint_path\n","\t\t\tself.saver.restore(self.sess, load_path)\n","\t\texcept:\n","\t\t\tself.logger.write(\"no saved model to load. starting new session\")\n","\t\t\tload_was_success = False\n","\t\telse:\n","\t\t\tself.logger.write(\"loaded model: {}\".format(load_path))\n","\t\t\tself.saver = tf.train.Saver(tf.global_variables())\n","\t\t\tglobal_step = int(load_path.split('-')[-1])\n","\t\treturn load_was_success, global_step"],"metadata":{"id":"CKtukFmoDD03"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# model training"],"metadata":{"id":"aQQP0Yv9jFvt"}},{"cell_type":"code","source":["def train_model(args):\n","\tlogger = Logger(args) # make logging utility\n","\tlogger.write(\"\\nTRAINING MODE...\")\n","\tlogger.write(\"{}\\n\".format(args))\n","\tlogger.write(\"loading data...\")\n","\tdata_loader = DataLoader(args, logger=logger)\n","\t\n","\tlogger.write(\"building model...\")\n","\tmodel = Model(args, logger=logger)\n","\n","\tlogger.write(\"attempt to load saved model...\")\n","\tload_was_success, global_step = model.try_load_model(args.save_path)\n","\n","\tv_x, v_y, v_s, v_c = data_loader.validation_data()\n","\tvalid_inputs = {model.input_data: v_x, model.target_data: v_y, model.char_seq: v_c}\n","\n","\tlogger.write(\"training...\")\n","\tmodel.sess.run(tf.assign(model.decay, args.decay ))\n","\tmodel.sess.run(tf.assign(model.momentum, args.momentum ))\n","\trunning_average = 0.0 ; remember_rate = 0.99\n","\tfor e in range(global_step//args.nbatches, args.nepochs):\n","\t\tmodel.sess.run(tf.assign(model.learning_rate, args.learning_rate * (args.lr_decay ** e)))\n","\t\tlogger.write(\"learning rate: {}\".format(model.learning_rate.eval()))\n","\n","\t\tc0, c1, c2 = model.istate_cell0.c.eval(), model.istate_cell1.c.eval(), model.istate_cell2.c.eval()\n","\t\th0, h1, h2 = model.istate_cell0.h.eval(), model.istate_cell1.h.eval(), model.istate_cell2.h.eval()\n","\t\tkappa = np.zeros((args.batch_size, args.kmixtures, 1))\n","\n","\t\tfor b in range(global_step%args.nbatches, args.nbatches):\n","\t\t\t\n","\t\t\ti = e * args.nbatches + b\n","\t\t\tif global_step is not 0 : i+=1 ; global_step = 0\n","\n","\t\t\tif i % args.save_every == 0 and (i > 0):\n","\t\t\t\tmodel.saver.save(model.sess, args.save_path, global_step = i) ; logger.write('SAVED MODEL')\n","\n","\t\t\tstart = time.time()\n","\t\t\tx, y, s, c = data_loader.next_batch()\n","\n","\t\t\tfeed = {model.input_data: x, model.target_data: y, model.char_seq: c, model.init_kappa: kappa, \\\n","\t\t\t\t\tmodel.istate_cell0.c: c0, model.istate_cell1.c: c1, model.istate_cell2.c: c2, \\\n","\t\t\t\t\tmodel.istate_cell0.h: h0, model.istate_cell1.h: h1, model.istate_cell2.h: h2}\n","\n","\t\t\t[train_loss, _] = model.sess.run([model.cost, model.train_op], feed)\n","\t\t\tfeed.update(valid_inputs)\n","\t\t\tfeed[model.init_kappa] = np.zeros((args.batch_size, args.kmixtures, 1))\n","\t\t\t[valid_loss] = model.sess.run([model.cost], feed)\n","\t\t\t\n","\t\t\trunning_average = running_average*remember_rate + train_loss*(1-remember_rate)\n","\n","\t\t\tend = time.time()\n","\t\t\tif i % 10 is 0: logger.write(\"{}/{}, loss = {:.3f}, regloss = {:.5f}, valid_loss = {:.3f}, time = {:.3f}\" \\\n","\t\t\t\t.format(i, args.nepochs * args.nbatches, train_loss, running_average, valid_loss, end - start) )"],"metadata":{"id":"sXU_0b668auo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# sample creation"],"metadata":{"id":"iONd30VOjK1Q"}},{"cell_type":"code","source":["from sample import *\n","def sample_model(args, logger=None):\n","\tif args.text == '':\n","\t\tstrings = ['call me ishmael some years ago', 'A project by Sam Greydanus', 'mmm mmm mmm mmm mmm mmm mmm', \\\n","\t\t\t'What I cannot create I do not understand', 'You know nothing Jon Snow'] # test strings\n","\telse:\n","\t\tstrings = [args.text]\n","\n","\tlogger = Logger(args) if logger is None else logger # instantiate logger\n","\tlogger.write(\"\\nSAMPLING MODE...\")\n","\tlogger.write(\"loading data...\")\n","\t\n","\tlogger.write(\"building model...\")\n","\tmodel = Model(args, logger)\n","\n","\tlogger.write(\"attempt to load saved model...\")\n","\tload_was_success, global_step = model.try_load_model(args.save_path)\n","\n","\tif load_was_success:\n","\t\tfor s in strings:\n","\t\t\tstrokes, phis, windows, kappas = sample(s, model, args)\n","\n","\t\t\tw_save_path = '{}figures/iter-{}-w-{}'.format(args.log_dir, global_step, s[:10].replace(' ', '_'))\n","\t\t\tg_save_path = '{}figures/iter-{}-g-{}'.format(args.log_dir, global_step, s[:10].replace(' ', '_'))\n","\t\t\tl_save_path = '{}figures/iter-{}-l-{}'.format(args.log_dir, global_step, s[:10].replace(' ', '_'))\n","\n","\t\t\twindow_plots(phis, windows, save_path=w_save_path)\n","\t\t\tgauss_plot(strokes, 'Heatmap for \"{}\"'.format(s), figsize = (2*len(s),4), save_path=g_save_path)\n","\t\t\tline_plot(strokes, 'Line plot for \"{}\"'.format(s), figsize = (len(s),2), save_path=l_save_path)\n","\n","\t\t\t# make sure that kappas are reasonable\n","\t\t\tlogger.write( \"kappas: \\n{}\".format(str(kappas[min(kappas.shape[0]-1, args.tsteps_per_ascii),:])) )\n","\telse:\n","\t\tlogger.write(\"load failed, sampling canceled\")\n","\n","\tif True:\n","\t\ttf.reset_default_graph()\n","\t\ttime.sleep(args.sleep_time)\n","\t\tsample_model(args, logger=logger)"],"metadata":{"id":"xHITcwp3_2Kl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.reset_default_graph()\n","#importlib.reload(utils.py)\n","#from utils import *\n","class Args():\n","  def __init__(self):\n","    #general model params\n","    self.train=True\n","    self.rnn_size = 100\n","    self.tsteps = 150\n","    self.nmixtures = 8\n","\n","    #indow params\n","    self.kmixtures = 1\n","    self.alphabet=\" abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n","    self.tsteps_per_ascii = 25\n","\n","    #training params\n","    self.batch_size=32\n","    self.nbatches=int(500)\n","    self.nepochs=int(250)\n","    self.dropout = 0.85\n","\n","\n","    self.grad_clip = 10.\n","    self.optimizer='rmsprop'\n","    self.learning_rate=1e-4\n","    self.lr_decay=1.0\n","    self.data_dir = './data'\n","    self.decay=0.95\n","    self.momentum = 0.9\n","\n","\n","    self.log_dir='./logs'\n","    self.data_scale=50\n","    self.data_dir='./data'\n","    self.save_path='./model2/savedModel.ckpt'\n","    self.save_every=1000\n","    \n","\n","    self.text=\"Hello this is Anu\"\n","    self.bias=1.0\n","    self.sleep_time=60*5\n","    self.style=-1\n","\n","\n","\n","\n","args = Args()\n","train_model(args)\n","#sample_model(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"RrInb9f28wHR","outputId":"cf11e70e-143b-433b-cda6-aa9edf96f0cb","executionInfo":{"status":"error","timestamp":1641201372546,"user_tz":-330,"elapsed":2217316,"user":{"displayName":"Anu Sree","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7pQ0bmhnHiKFrTiED7uuMcCb0zd-5OOYmVT9-dg=s64","userId":"05137986881356860231"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","TRAINING MODE...\n","<__main__.Args object at 0x7f1ad473c190>\n","\n","loading data...\n","\tloaded dataset:\n","\t\t11262 train individual data points\n","\t\t592 valid individual data points\n","\t\t351 batches\n","building model...\n","\tusing alphabet abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n"]},{"output_type":"stream","name":"stderr","text":["/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n","  warnings.warn('An interactive session is already active. This can '\n"]},{"output_type":"stream","name":"stdout","text":["attempt to load saved model...\n","no saved model to load. starting new session\n","training...\n","learning rate: 9.999999747378752e-05\n","0/125000, loss = 3.090, regloss = 0.03090, valid_loss = 3.061, time = 46.975\n","10/125000, loss = 3.106, regloss = 0.31705, valid_loss = 3.056, time = 0.446\n","20/125000, loss = 3.039, regloss = 0.57752, valid_loss = 3.046, time = 0.454\n","30/125000, loss = 2.912, regloss = 0.81331, valid_loss = 3.033, time = 0.446\n","40/125000, loss = 3.039, regloss = 1.02522, valid_loss = 3.014, time = 0.823\n","50/125000, loss = 2.982, regloss = 1.21226, valid_loss = 2.990, time = 0.444\n","60/125000, loss = 3.012, regloss = 1.37867, valid_loss = 2.960, time = 0.446\n","70/125000, loss = 2.879, regloss = 1.52687, valid_loss = 2.917, time = 0.447\n","80/125000, loss = 2.799, regloss = 1.65591, valid_loss = 2.855, time = 0.459\n","90/125000, loss = 2.761, regloss = 1.76637, valid_loss = 2.763, time = 0.446\n","100/125000, loss = 2.564, regloss = 1.85541, valid_loss = 2.612, time = 0.449\n","110/125000, loss = 2.413, regloss = 1.91864, valid_loss = 2.374, time = 0.445\n","120/125000, loss = 2.009, regloss = 1.94675, valid_loss = 2.033, time = 0.448\n","130/125000, loss = 1.797, regloss = 1.93835, valid_loss = 1.641, time = 0.451\n","140/125000, loss = 1.370, regloss = 1.89288, valid_loss = 1.328, time = 0.450\n","150/125000, loss = 1.228, regloss = 1.83421, valid_loss = 1.215, time = 0.449\n","160/125000, loss = 1.135, regloss = 1.77962, valid_loss = 1.181, time = 0.454\n","170/125000, loss = 1.130, regloss = 1.72817, valid_loss = 1.157, time = 0.448\n","180/125000, loss = 1.122, regloss = 1.67427, valid_loss = 1.138, time = 0.447\n","190/125000, loss = 1.305, regloss = 1.62344, valid_loss = 1.127, time = 0.447\n","200/125000, loss = 1.273, regloss = 1.57800, valid_loss = 1.117, time = 0.449\n","210/125000, loss = 1.025, regloss = 1.53628, valid_loss = 1.112, time = 0.449\n","220/125000, loss = 1.255, regloss = 1.49815, valid_loss = 1.096, time = 0.451\n","230/125000, loss = 1.138, regloss = 1.46097, valid_loss = 1.080, time = 0.457\n","240/125000, loss = 0.943, regloss = 1.43056, valid_loss = 1.066, time = 0.452\n","250/125000, loss = 1.112, regloss = 1.39024, valid_loss = 1.029, time = 0.449\n","260/125000, loss = 1.159, regloss = 1.36187, valid_loss = 0.989, time = 0.453\n","270/125000, loss = 1.097, regloss = 1.32472, valid_loss = 0.903, time = 0.454\n","280/125000, loss = 0.937, regloss = 1.29041, valid_loss = 0.868, time = 0.452\n","290/125000, loss = 0.849, regloss = 1.25163, valid_loss = 0.807, time = 0.452\n","300/125000, loss = 0.836, regloss = 1.20896, valid_loss = 0.721, time = 0.451\n","310/125000, loss = 0.767, regloss = 1.16548, valid_loss = 0.660, time = 0.452\n","320/125000, loss = 0.407, regloss = 1.11489, valid_loss = 0.543, time = 0.461\n","330/125000, loss = 0.504, regloss = 1.05394, valid_loss = 0.455, time = 0.451\n","340/125000, loss = 0.347, regloss = 0.99253, valid_loss = 0.358, time = 0.451\n","350/125000, loss = 0.039, regloss = 0.92049, valid_loss = 0.219, time = 0.452\n","360/125000, loss = 0.207, regloss = 0.85002, valid_loss = 0.070, time = 0.451\n","370/125000, loss = 0.049, regloss = 0.77839, valid_loss = -0.067, time = 0.450\n","380/125000, loss = -0.261, regloss = 0.69442, valid_loss = -0.190, time = 0.457\n","390/125000, loss = -0.284, regloss = 0.60300, valid_loss = -0.318, time = 0.451\n","400/125000, loss = -0.373, regloss = 0.51245, valid_loss = -0.371, time = 0.448\n","410/125000, loss = -0.525, regloss = 0.42261, valid_loss = -0.484, time = 0.449\n","420/125000, loss = -0.539, regloss = 0.32976, valid_loss = -0.579, time = 0.448\n","430/125000, loss = -0.740, regloss = 0.24319, valid_loss = -0.618, time = 0.454\n","440/125000, loss = -0.619, regloss = 0.16230, valid_loss = -0.674, time = 0.446\n","450/125000, loss = -0.921, regloss = 0.08184, valid_loss = -0.723, time = 0.455\n","460/125000, loss = -0.797, regloss = 0.00167, valid_loss = -0.765, time = 0.446\n","470/125000, loss = -0.870, regloss = -0.06796, valid_loss = -0.810, time = 0.446\n","480/125000, loss = -0.834, regloss = -0.14047, valid_loss = -0.830, time = 0.450\n","490/125000, loss = -0.841, regloss = -0.20494, valid_loss = -0.854, time = 0.451\n","learning rate: 9.999999747378752e-05\n","500/125000, loss = -0.961, regloss = -0.26492, valid_loss = -0.896, time = 0.448\n","510/125000, loss = -0.813, regloss = -0.31993, valid_loss = -0.906, time = 0.454\n","520/125000, loss = -0.825, regloss = -0.37398, valid_loss = -0.963, time = 0.447\n","530/125000, loss = -0.935, regloss = -0.42458, valid_loss = -0.980, time = 0.456\n","540/125000, loss = -0.924, regloss = -0.47252, valid_loss = -0.999, time = 0.456\n","550/125000, loss = -0.868, regloss = -0.51989, valid_loss = -1.026, time = 0.446\n","560/125000, loss = -1.019, regloss = -0.56490, valid_loss = -1.059, time = 0.452\n","570/125000, loss = -0.931, regloss = -0.60434, valid_loss = -1.087, time = 0.455\n","580/125000, loss = -1.043, regloss = -0.65041, valid_loss = -1.098, time = 0.448\n","590/125000, loss = -1.049, regloss = -0.69075, valid_loss = -1.134, time = 0.448\n","600/125000, loss = -1.077, regloss = -0.72839, valid_loss = -1.124, time = 0.449\n","610/125000, loss = -1.064, regloss = -0.77154, valid_loss = -1.130, time = 0.452\n","620/125000, loss = -0.997, regloss = -0.80106, valid_loss = -1.160, time = 0.452\n","630/125000, loss = -1.252, regloss = -0.83249, valid_loss = -1.168, time = 0.450\n","640/125000, loss = -0.867, regloss = -0.86269, valid_loss = -1.205, time = 0.456\n","650/125000, loss = -1.129, regloss = -0.88926, valid_loss = -1.213, time = 0.448\n","660/125000, loss = -1.210, regloss = -0.91138, valid_loss = -1.201, time = 0.449\n","670/125000, loss = -1.096, regloss = -0.93248, valid_loss = -1.226, time = 0.449\n","680/125000, loss = -1.233, regloss = -0.95949, valid_loss = -1.220, time = 0.458\n","690/125000, loss = -1.266, regloss = -0.97899, valid_loss = -1.255, time = 0.447\n","700/125000, loss = -1.302, regloss = -0.99917, valid_loss = -1.229, time = 0.451\n","710/125000, loss = -1.220, regloss = -1.01235, valid_loss = -1.251, time = 0.448\n","720/125000, loss = -1.319, regloss = -1.03351, valid_loss = -1.269, time = 0.457\n","730/125000, loss = -1.333, regloss = -1.04883, valid_loss = -1.260, time = 0.448\n","740/125000, loss = -1.154, regloss = -1.06729, valid_loss = -1.283, time = 0.448\n","750/125000, loss = -1.268, regloss = -1.08062, valid_loss = -1.253, time = 0.448\n","760/125000, loss = -1.251, regloss = -1.09193, valid_loss = -1.268, time = 0.448\n","770/125000, loss = -1.257, regloss = -1.10668, valid_loss = -1.279, time = 0.449\n","780/125000, loss = -1.324, regloss = -1.12433, valid_loss = -1.301, time = 0.452\n","790/125000, loss = -1.161, regloss = -1.13269, valid_loss = -1.298, time = 0.448\n","800/125000, loss = -1.339, regloss = -1.14814, valid_loss = -1.300, time = 0.445\n","810/125000, loss = -1.155, regloss = -1.15166, valid_loss = -1.308, time = 0.449\n","820/125000, loss = -1.132, regloss = -1.15927, valid_loss = -1.290, time = 0.449\n","830/125000, loss = -1.287, regloss = -1.17039, valid_loss = -1.318, time = 0.456\n","840/125000, loss = -1.245, regloss = -1.17731, valid_loss = -1.308, time = 0.451\n","850/125000, loss = -1.217, regloss = -1.18167, valid_loss = -1.312, time = 0.447\n","860/125000, loss = -1.422, regloss = -1.19062, valid_loss = -1.336, time = 0.448\n","870/125000, loss = -1.262, regloss = -1.19766, valid_loss = -1.328, time = 0.459\n","880/125000, loss = -1.316, regloss = -1.20982, valid_loss = -1.346, time = 0.448\n","890/125000, loss = -1.404, regloss = -1.21877, valid_loss = -1.344, time = 0.448\n","900/125000, loss = -1.392, regloss = -1.22815, valid_loss = -1.350, time = 0.450\n","910/125000, loss = -1.328, regloss = -1.23159, valid_loss = -1.360, time = 0.450\n","920/125000, loss = -1.288, regloss = -1.23803, valid_loss = -1.358, time = 0.452\n","930/125000, loss = -1.378, regloss = -1.24563, valid_loss = -1.354, time = 0.453\n","940/125000, loss = -1.298, regloss = -1.25392, valid_loss = -1.376, time = 0.452\n","950/125000, loss = -1.379, regloss = -1.25797, valid_loss = -1.364, time = 0.450\n","960/125000, loss = -1.288, regloss = -1.26390, valid_loss = -1.364, time = 0.449\n","970/125000, loss = -1.324, regloss = -1.26671, valid_loss = -1.369, time = 0.450\n","980/125000, loss = -1.286, regloss = -1.27209, valid_loss = -1.378, time = 0.461\n","990/125000, loss = -1.355, regloss = -1.27423, valid_loss = -1.347, time = 0.450\n","learning rate: 9.999999747378752e-05\n","SAVED MODEL\n","1000/125000, loss = -1.327, regloss = -1.27823, valid_loss = -1.371, time = 0.458\n","1010/125000, loss = -1.169, regloss = -1.27944, valid_loss = -1.373, time = 0.460\n","1020/125000, loss = -1.156, regloss = -1.28293, valid_loss = -1.389, time = 0.448\n","1030/125000, loss = -1.322, regloss = -1.28993, valid_loss = -1.403, time = 0.448\n","1040/125000, loss = -1.387, regloss = -1.29029, valid_loss = -1.394, time = 0.451\n","1050/125000, loss = -1.419, regloss = -1.29986, valid_loss = -1.402, time = 0.457\n","1060/125000, loss = -1.410, regloss = -1.30580, valid_loss = -1.398, time = 0.447\n","1070/125000, loss = -1.243, regloss = -1.30964, valid_loss = -1.406, time = 0.452\n","1080/125000, loss = -1.365, regloss = -1.31136, valid_loss = -1.405, time = 0.447\n","1090/125000, loss = -1.358, regloss = -1.31813, valid_loss = -1.405, time = 0.448\n","1100/125000, loss = -1.421, regloss = -1.32609, valid_loss = -1.398, time = 0.450\n","1110/125000, loss = -1.367, regloss = -1.33080, valid_loss = -1.414, time = 0.449\n","1120/125000, loss = -1.340, regloss = -1.32988, valid_loss = -1.414, time = 0.452\n","1130/125000, loss = -1.327, regloss = -1.33193, valid_loss = -1.401, time = 0.451\n","1140/125000, loss = -1.405, regloss = -1.33623, valid_loss = -1.415, time = 0.450\n","1150/125000, loss = -1.334, regloss = -1.33641, valid_loss = -1.417, time = 0.459\n","1160/125000, loss = -1.394, regloss = -1.34220, valid_loss = -1.409, time = 0.460\n","1170/125000, loss = -1.404, regloss = -1.34416, valid_loss = -1.417, time = 0.451\n","1180/125000, loss = -1.275, regloss = -1.34485, valid_loss = -1.432, time = 0.453\n","1190/125000, loss = -1.477, regloss = -1.34758, valid_loss = -1.417, time = 0.460\n","1200/125000, loss = -1.350, regloss = -1.34790, valid_loss = -1.424, time = 0.459\n","1210/125000, loss = -1.259, regloss = -1.35097, valid_loss = -1.431, time = 0.455\n","1220/125000, loss = -1.309, regloss = -1.35168, valid_loss = -1.420, time = 0.452\n","1230/125000, loss = -1.476, regloss = -1.35813, valid_loss = -1.450, time = 0.455\n","1240/125000, loss = -1.269, regloss = -1.35753, valid_loss = -1.445, time = 0.454\n","1250/125000, loss = -1.312, regloss = -1.35580, valid_loss = -1.439, time = 0.450\n","1260/125000, loss = -1.464, regloss = -1.35754, valid_loss = -1.435, time = 0.449\n","1270/125000, loss = -1.524, regloss = -1.35994, valid_loss = -1.464, time = 0.451\n","1280/125000, loss = -1.116, regloss = -1.36312, valid_loss = -1.421, time = 0.454\n","1290/125000, loss = -1.442, regloss = -1.36703, valid_loss = -1.457, time = 0.447\n","1300/125000, loss = -1.332, regloss = -1.37215, valid_loss = -1.469, time = 0.448\n","1310/125000, loss = -1.370, regloss = -1.37409, valid_loss = -1.468, time = 0.457\n","1320/125000, loss = -1.309, regloss = -1.37523, valid_loss = -1.455, time = 0.452\n","1330/125000, loss = -1.294, regloss = -1.37782, valid_loss = -1.468, time = 0.454\n","1340/125000, loss = -1.347, regloss = -1.37812, valid_loss = -1.462, time = 0.462\n","1350/125000, loss = -1.481, regloss = -1.38268, valid_loss = -1.474, time = 0.453\n","1360/125000, loss = -1.315, regloss = -1.38804, valid_loss = -1.471, time = 0.448\n","1370/125000, loss = -1.363, regloss = -1.38945, valid_loss = -1.466, time = 0.454\n","1380/125000, loss = -1.085, regloss = -1.38687, valid_loss = -1.462, time = 0.450\n","1390/125000, loss = -1.507, regloss = -1.38589, valid_loss = -1.481, time = 0.565\n","1400/125000, loss = -1.512, regloss = -1.39004, valid_loss = -1.479, time = 0.562\n","1410/125000, loss = -1.479, regloss = -1.39195, valid_loss = -1.470, time = 0.483\n","1420/125000, loss = -1.364, regloss = -1.39408, valid_loss = -1.484, time = 0.455\n","1430/125000, loss = -1.499, regloss = -1.39809, valid_loss = -1.476, time = 0.454\n","1440/125000, loss = -1.464, regloss = -1.39772, valid_loss = -1.476, time = 0.449\n","1450/125000, loss = -1.608, regloss = -1.39862, valid_loss = -1.489, time = 0.447\n","1460/125000, loss = -1.387, regloss = -1.40150, valid_loss = -1.476, time = 0.449\n","1470/125000, loss = -1.523, regloss = -1.40439, valid_loss = -1.491, time = 0.450\n","1480/125000, loss = -1.430, regloss = -1.40335, valid_loss = -1.493, time = 0.447\n","1490/125000, loss = -1.601, regloss = -1.40500, valid_loss = -1.479, time = 0.447\n","learning rate: 9.999999747378752e-05\n","1500/125000, loss = -1.446, regloss = -1.40366, valid_loss = -1.478, time = 0.457\n","1510/125000, loss = -1.302, regloss = -1.40958, valid_loss = -1.487, time = 0.445\n","1520/125000, loss = -1.537, regloss = -1.41368, valid_loss = -1.506, time = 0.451\n","1530/125000, loss = -1.614, regloss = -1.41877, valid_loss = -1.493, time = 0.449\n","1540/125000, loss = -1.408, regloss = -1.41838, valid_loss = -1.481, time = 0.452\n","1550/125000, loss = -1.459, regloss = -1.42009, valid_loss = -1.493, time = 0.451\n","1560/125000, loss = -1.641, regloss = -1.42403, valid_loss = -1.505, time = 0.451\n","1570/125000, loss = -1.353, regloss = -1.42331, valid_loss = -1.511, time = 0.453\n","1580/125000, loss = -1.391, regloss = -1.42422, valid_loss = -1.510, time = 0.447\n","1590/125000, loss = -1.483, regloss = -1.42608, valid_loss = -1.494, time = 0.449\n","1600/125000, loss = -1.329, regloss = -1.43056, valid_loss = -1.508, time = 0.449\n","1610/125000, loss = -1.509, regloss = -1.43140, valid_loss = -1.509, time = 0.455\n","1620/125000, loss = -1.415, regloss = -1.43012, valid_loss = -1.508, time = 0.446\n","1630/125000, loss = -1.473, regloss = -1.43577, valid_loss = -1.486, time = 0.449\n","1640/125000, loss = -1.432, regloss = -1.43864, valid_loss = -1.511, time = 0.448\n","1650/125000, loss = -1.412, regloss = -1.44149, valid_loss = -1.512, time = 0.459\n","1660/125000, loss = -1.610, regloss = -1.44319, valid_loss = -1.532, time = 0.456\n","1670/125000, loss = -1.450, regloss = -1.44006, valid_loss = -1.540, time = 0.453\n","1680/125000, loss = -1.648, regloss = -1.44179, valid_loss = -1.512, time = 0.449\n","1690/125000, loss = -1.501, regloss = -1.44486, valid_loss = -1.506, time = 0.457\n","1700/125000, loss = -1.534, regloss = -1.45177, valid_loss = -1.527, time = 0.452\n","1710/125000, loss = -1.647, regloss = -1.45651, valid_loss = -1.525, time = 0.452\n","1720/125000, loss = -1.448, regloss = -1.45330, valid_loss = -1.540, time = 0.457\n","1730/125000, loss = -1.433, regloss = -1.45376, valid_loss = -1.529, time = 0.452\n","1740/125000, loss = -1.459, regloss = -1.45429, valid_loss = -1.532, time = 0.455\n","1750/125000, loss = -1.393, regloss = -1.45616, valid_loss = -1.522, time = 0.454\n","1760/125000, loss = -1.381, regloss = -1.45453, valid_loss = -1.528, time = 0.461\n","1770/125000, loss = -1.336, regloss = -1.45716, valid_loss = -1.541, time = 0.451\n","1780/125000, loss = -1.555, regloss = -1.45893, valid_loss = -1.532, time = 0.455\n","1790/125000, loss = -1.334, regloss = -1.45912, valid_loss = -1.518, time = 0.454\n","1800/125000, loss = -1.362, regloss = -1.46046, valid_loss = -1.532, time = 0.463\n","1810/125000, loss = -1.366, regloss = -1.45697, valid_loss = -1.531, time = 0.449\n","1820/125000, loss = -1.476, regloss = -1.46255, valid_loss = -1.549, time = 0.461\n","1830/125000, loss = -1.388, regloss = -1.46377, valid_loss = -1.530, time = 0.449\n","1840/125000, loss = -1.610, regloss = -1.46962, valid_loss = -1.544, time = 0.459\n","1850/125000, loss = -1.424, regloss = -1.46650, valid_loss = -1.546, time = 0.448\n","1860/125000, loss = -1.375, regloss = -1.46748, valid_loss = -1.546, time = 0.449\n","1870/125000, loss = -1.576, regloss = -1.47238, valid_loss = -1.534, time = 0.449\n","1880/125000, loss = -1.555, regloss = -1.47446, valid_loss = -1.556, time = 0.453\n","1890/125000, loss = -1.531, regloss = -1.48080, valid_loss = -1.548, time = 0.448\n","1900/125000, loss = -1.294, regloss = -1.48086, valid_loss = -1.539, time = 0.451\n","1910/125000, loss = -1.548, regloss = -1.48007, valid_loss = -1.552, time = 0.450\n","1920/125000, loss = -1.576, regloss = -1.48135, valid_loss = -1.555, time = 0.447\n","1930/125000, loss = -1.578, regloss = -1.48119, valid_loss = -1.558, time = 0.449\n","1940/125000, loss = -1.401, regloss = -1.48424, valid_loss = -1.553, time = 0.451\n","1950/125000, loss = -1.586, regloss = -1.48377, valid_loss = -1.553, time = 0.462\n","1960/125000, loss = -1.390, regloss = -1.48805, valid_loss = -1.561, time = 0.451\n","1970/125000, loss = -1.600, regloss = -1.48913, valid_loss = -1.559, time = 0.451\n","1980/125000, loss = -1.377, regloss = -1.48605, valid_loss = -1.550, time = 0.451\n","1990/125000, loss = -1.639, regloss = -1.49130, valid_loss = -1.552, time = 0.457\n","learning rate: 9.999999747378752e-05\n","SAVED MODEL\n","2000/125000, loss = -1.456, regloss = -1.49091, valid_loss = -1.536, time = 0.447\n","2010/125000, loss = -1.531, regloss = -1.49227, valid_loss = -1.549, time = 0.456\n","2020/125000, loss = -1.589, regloss = -1.49900, valid_loss = -1.561, time = 0.459\n","2030/125000, loss = -1.440, regloss = -1.49209, valid_loss = -1.553, time = 0.449\n","2040/125000, loss = -1.382, regloss = -1.49116, valid_loss = -1.575, time = 0.452\n","2050/125000, loss = -1.309, regloss = -1.48980, valid_loss = -1.568, time = 0.452\n","2060/125000, loss = -1.373, regloss = -1.48900, valid_loss = -1.586, time = 0.458\n","2070/125000, loss = -1.445, regloss = -1.49060, valid_loss = -1.564, time = 0.446\n","2080/125000, loss = -1.462, regloss = -1.49350, valid_loss = -1.568, time = 0.449\n","2090/125000, loss = -1.699, regloss = -1.49188, valid_loss = -1.559, time = 0.447\n","2100/125000, loss = -1.486, regloss = -1.49814, valid_loss = -1.569, time = 0.453\n","2110/125000, loss = -1.656, regloss = -1.50186, valid_loss = -1.571, time = 0.447\n","2120/125000, loss = -1.333, regloss = -1.49721, valid_loss = -1.564, time = 0.446\n","2130/125000, loss = -1.533, regloss = -1.49828, valid_loss = -1.585, time = 0.450\n","2140/125000, loss = -1.286, regloss = -1.49696, valid_loss = -1.579, time = 0.447\n","2150/125000, loss = -1.525, regloss = -1.49983, valid_loss = -1.592, time = 0.450\n","2160/125000, loss = -1.590, regloss = -1.50801, valid_loss = -1.574, time = 0.445\n","2170/125000, loss = -1.618, regloss = -1.50893, valid_loss = -1.581, time = 0.449\n","2180/125000, loss = -1.579, regloss = -1.51278, valid_loss = -1.596, time = 0.446\n","2190/125000, loss = -1.579, regloss = -1.51603, valid_loss = -1.592, time = 0.445\n","2200/125000, loss = -1.732, regloss = -1.52152, valid_loss = -1.615, time = 0.446\n","2210/125000, loss = -1.684, regloss = -1.52143, valid_loss = -1.612, time = 0.452\n","2220/125000, loss = -1.640, regloss = -1.52460, valid_loss = -1.600, time = 0.445\n","2230/125000, loss = -1.532, regloss = -1.52565, valid_loss = -1.618, time = 0.448\n","2240/125000, loss = -1.499, regloss = -1.52914, valid_loss = -1.596, time = 0.452\n","2250/125000, loss = -1.409, regloss = -1.53114, valid_loss = -1.621, time = 0.445\n","2260/125000, loss = -1.535, regloss = -1.53186, valid_loss = -1.602, time = 0.450\n","2270/125000, loss = -1.323, regloss = -1.53154, valid_loss = -1.587, time = 0.448\n","2280/125000, loss = -1.518, regloss = -1.52950, valid_loss = -1.617, time = 0.446\n","2290/125000, loss = -1.613, regloss = -1.52940, valid_loss = -1.581, time = 0.444\n","2300/125000, loss = -1.479, regloss = -1.52895, valid_loss = -1.629, time = 0.448\n","2310/125000, loss = -1.639, regloss = -1.52988, valid_loss = -1.576, time = 0.449\n","2320/125000, loss = -1.467, regloss = -1.52892, valid_loss = -1.584, time = 0.450\n","2330/125000, loss = -1.431, regloss = -1.52805, valid_loss = -1.628, time = 0.448\n","2340/125000, loss = -1.713, regloss = -1.53871, valid_loss = -1.625, time = 0.448\n","2350/125000, loss = -1.604, regloss = -1.54266, valid_loss = -1.622, time = 0.445\n","2360/125000, loss = -1.527, regloss = -1.54445, valid_loss = -1.605, time = 0.453\n","2370/125000, loss = -1.674, regloss = -1.54720, valid_loss = -1.602, time = 0.446\n","2380/125000, loss = -1.718, regloss = -1.54856, valid_loss = -1.588, time = 0.446\n","2390/125000, loss = -1.517, regloss = -1.54764, valid_loss = -1.593, time = 0.445\n","2400/125000, loss = -1.417, regloss = -1.54358, valid_loss = -1.525, time = 0.449\n","2410/125000, loss = -1.661, regloss = -1.53658, valid_loss = -1.596, time = 0.445\n","2420/125000, loss = -1.695, regloss = -1.54106, valid_loss = -1.610, time = 0.455\n","2430/125000, loss = -1.523, regloss = -1.53964, valid_loss = -1.636, time = 0.445\n","2440/125000, loss = -1.508, regloss = -1.54690, valid_loss = -1.617, time = 0.445\n","2450/125000, loss = -1.657, regloss = -1.54242, valid_loss = -1.611, time = 0.447\n","2460/125000, loss = -1.612, regloss = -1.55091, valid_loss = -1.635, time = 0.445\n","2470/125000, loss = -1.676, regloss = -1.56121, valid_loss = -1.608, time = 0.450\n","2480/125000, loss = -1.708, regloss = -1.56086, valid_loss = -1.611, time = 0.444\n","2490/125000, loss = -1.694, regloss = -1.56288, valid_loss = -1.614, time = 0.452\n","learning rate: 9.999999747378752e-05\n","2500/125000, loss = -1.636, regloss = -1.56867, valid_loss = -1.543, time = 0.453\n","2510/125000, loss = -1.578, regloss = -1.56765, valid_loss = -1.589, time = 0.463\n","2520/125000, loss = -1.445, regloss = -1.56614, valid_loss = -1.618, time = 0.454\n","2530/125000, loss = -1.620, regloss = -1.56665, valid_loss = -1.572, time = 0.455\n","2540/125000, loss = -1.603, regloss = -1.56471, valid_loss = -1.613, time = 0.453\n","2550/125000, loss = -1.714, regloss = -1.56515, valid_loss = -1.617, time = 0.459\n","2560/125000, loss = -1.531, regloss = -1.56574, valid_loss = -1.648, time = 0.451\n","2570/125000, loss = -1.583, regloss = -1.56379, valid_loss = -1.587, time = 0.451\n","2580/125000, loss = -1.369, regloss = -1.55872, valid_loss = -1.654, time = 0.451\n","2590/125000, loss = -1.525, regloss = -1.56573, valid_loss = -1.653, time = 0.451\n","2600/125000, loss = -1.595, regloss = -1.56742, valid_loss = -1.644, time = 0.450\n","2610/125000, loss = -1.605, regloss = -1.57027, valid_loss = -1.636, time = 0.449\n","2620/125000, loss = -1.648, regloss = -1.57222, valid_loss = -1.654, time = 0.456\n","2630/125000, loss = -1.545, regloss = -1.57527, valid_loss = -1.621, time = 0.449\n","2640/125000, loss = -1.428, regloss = -1.57401, valid_loss = -1.646, time = 0.451\n","2650/125000, loss = -1.690, regloss = -1.57282, valid_loss = -1.645, time = 0.456\n","2660/125000, loss = -1.565, regloss = -1.57208, valid_loss = -1.626, time = 0.455\n","2670/125000, loss = -1.592, regloss = -1.57434, valid_loss = -1.634, time = 0.449\n","2680/125000, loss = -1.527, regloss = -1.57450, valid_loss = -1.636, time = 0.446\n","2690/125000, loss = -1.481, regloss = -1.57371, valid_loss = -1.629, time = 0.454\n","2700/125000, loss = -1.563, regloss = -1.57041, valid_loss = -1.635, time = 0.450\n","2710/125000, loss = -1.451, regloss = -1.56513, valid_loss = -1.597, time = 0.446\n","2720/125000, loss = -1.473, regloss = -1.56601, valid_loss = -1.628, time = 0.445\n","2730/125000, loss = -1.601, regloss = -1.56534, valid_loss = -1.626, time = 0.446\n","2740/125000, loss = -1.728, regloss = -1.56761, valid_loss = -1.602, time = 0.450\n","2750/125000, loss = -1.553, regloss = -1.56828, valid_loss = -1.628, time = 0.449\n","2760/125000, loss = -1.550, regloss = -1.57122, valid_loss = -1.651, time = 0.451\n","2770/125000, loss = -1.617, regloss = -1.57238, valid_loss = -1.661, time = 0.467\n","2780/125000, loss = -1.562, regloss = -1.57397, valid_loss = -1.634, time = 0.446\n","2790/125000, loss = -1.673, regloss = -1.57912, valid_loss = -1.645, time = 0.453\n","2800/125000, loss = -1.648, regloss = -1.58346, valid_loss = -1.659, time = 0.447\n","2810/125000, loss = -1.465, regloss = -1.58199, valid_loss = -1.641, time = 0.456\n","2820/125000, loss = -1.479, regloss = -1.58330, valid_loss = -1.662, time = 0.447\n","2830/125000, loss = -1.748, regloss = -1.58844, valid_loss = -1.677, time = 0.447\n","2840/125000, loss = -1.688, regloss = -1.59539, valid_loss = -1.645, time = 0.446\n","2850/125000, loss = -1.552, regloss = -1.59100, valid_loss = -1.666, time = 0.455\n","2860/125000, loss = -1.472, regloss = -1.58702, valid_loss = -1.663, time = 0.460\n","2870/125000, loss = -1.608, regloss = -1.59449, valid_loss = -1.647, time = 0.448\n","2880/125000, loss = -1.618, regloss = -1.59320, valid_loss = -1.661, time = 0.448\n","2890/125000, loss = -1.803, regloss = -1.59591, valid_loss = -1.678, time = 0.450\n","2900/125000, loss = -1.490, regloss = -1.60015, valid_loss = -1.658, time = 0.446\n","2910/125000, loss = -1.679, regloss = -1.60188, valid_loss = -1.657, time = 0.449\n","2920/125000, loss = -1.615, regloss = -1.59979, valid_loss = -1.673, time = 0.453\n","2930/125000, loss = -1.607, regloss = -1.60377, valid_loss = -1.629, time = 0.446\n","2940/125000, loss = -1.681, regloss = -1.60704, valid_loss = -1.676, time = 0.449\n","2950/125000, loss = -1.715, regloss = -1.61307, valid_loss = -1.675, time = 0.451\n","2960/125000, loss = -1.543, regloss = -1.61279, valid_loss = -1.655, time = 0.460\n","2970/125000, loss = -1.562, regloss = -1.60934, valid_loss = -1.668, time = 0.447\n","2980/125000, loss = -1.541, regloss = -1.61021, valid_loss = -1.676, time = 0.451\n","2990/125000, loss = -1.676, regloss = -1.61024, valid_loss = -1.652, time = 0.457\n","learning rate: 9.999999747378752e-05\n","SAVED MODEL\n","3000/125000, loss = -1.672, regloss = -1.61110, valid_loss = -1.669, time = 0.446\n","3010/125000, loss = -1.659, regloss = -1.61266, valid_loss = -1.665, time = 0.466\n","3020/125000, loss = -1.656, regloss = -1.61032, valid_loss = -1.654, time = 0.449\n","3030/125000, loss = -1.768, regloss = -1.60962, valid_loss = -1.679, time = 0.456\n","3040/125000, loss = -1.652, regloss = -1.61024, valid_loss = -1.672, time = 0.452\n","3050/125000, loss = -1.725, regloss = -1.61402, valid_loss = -1.666, time = 0.447\n","3060/125000, loss = -1.735, regloss = -1.61601, valid_loss = -1.678, time = 0.449\n","3070/125000, loss = -1.596, regloss = -1.61199, valid_loss = -1.671, time = 0.451\n","3080/125000, loss = -1.598, regloss = -1.60807, valid_loss = -1.671, time = 0.449\n","3090/125000, loss = -1.606, regloss = -1.60685, valid_loss = -1.679, time = 0.454\n","3100/125000, loss = -1.657, regloss = -1.61142, valid_loss = -1.691, time = 0.456\n","3110/125000, loss = -1.664, regloss = -1.61085, valid_loss = -1.681, time = 0.463\n","3120/125000, loss = -1.736, regloss = -1.61431, valid_loss = -1.668, time = 0.453\n","3130/125000, loss = -1.560, regloss = -1.61370, valid_loss = -1.669, time = 0.447\n","3140/125000, loss = -1.504, regloss = -1.61241, valid_loss = -1.678, time = 0.450\n","3150/125000, loss = -1.488, regloss = -1.61289, valid_loss = -1.702, time = 0.458\n","3160/125000, loss = -1.625, regloss = -1.61283, valid_loss = -1.697, time = 0.450\n","3170/125000, loss = -1.663, regloss = -1.60859, valid_loss = -1.694, time = 0.457\n","3180/125000, loss = -1.803, regloss = -1.61323, valid_loss = -1.688, time = 0.455\n","3190/125000, loss = -1.792, regloss = -1.61306, valid_loss = -1.673, time = 0.463\n","3200/125000, loss = -1.519, regloss = -1.60594, valid_loss = -1.701, time = 0.458\n","3210/125000, loss = -1.640, regloss = -1.60985, valid_loss = -1.695, time = 0.453\n","3220/125000, loss = -1.585, regloss = -1.61124, valid_loss = -1.692, time = 0.450\n","3230/125000, loss = -1.575, regloss = -1.61558, valid_loss = -1.697, time = 0.452\n","3240/125000, loss = -1.825, regloss = -1.61961, valid_loss = -1.662, time = 0.454\n","3250/125000, loss = -1.592, regloss = -1.62546, valid_loss = -1.678, time = 0.452\n","3260/125000, loss = -1.555, regloss = -1.62609, valid_loss = -1.642, time = 0.457\n","3270/125000, loss = -1.651, regloss = -1.62688, valid_loss = -1.684, time = 0.454\n","3280/125000, loss = -1.414, regloss = -1.62357, valid_loss = -1.688, time = 0.452\n","3290/125000, loss = -1.662, regloss = -1.62340, valid_loss = -1.707, time = 0.452\n","3300/125000, loss = -1.566, regloss = -1.62720, valid_loss = -1.710, time = 0.461\n","3310/125000, loss = -1.701, regloss = -1.63406, valid_loss = -1.724, time = 0.449\n","3320/125000, loss = -1.830, regloss = -1.63787, valid_loss = -1.716, time = 0.454\n","3330/125000, loss = -1.751, regloss = -1.64057, valid_loss = -1.724, time = 0.456\n","3340/125000, loss = -1.644, regloss = -1.64151, valid_loss = -1.739, time = 0.462\n","3350/125000, loss = -1.587, regloss = -1.63993, valid_loss = -1.703, time = 0.448\n","3360/125000, loss = -1.745, regloss = -1.64279, valid_loss = -1.711, time = 0.451\n","3370/125000, loss = -1.651, regloss = -1.64540, valid_loss = -1.725, time = 0.448\n","3380/125000, loss = -1.493, regloss = -1.64242, valid_loss = -1.589, time = 0.449\n","3390/125000, loss = -1.566, regloss = -1.63748, valid_loss = -1.664, time = 0.455\n","3400/125000, loss = -1.700, regloss = -1.63684, valid_loss = -1.670, time = 0.452\n","3410/125000, loss = -1.710, regloss = -1.63698, valid_loss = -1.669, time = 0.459\n","3420/125000, loss = -1.557, regloss = -1.63535, valid_loss = -1.711, time = 0.446\n","3430/125000, loss = -1.805, regloss = -1.63782, valid_loss = -1.709, time = 0.446\n","3440/125000, loss = -1.632, regloss = -1.63761, valid_loss = -1.642, time = 0.449\n","3450/125000, loss = -1.588, regloss = -1.63733, valid_loss = -1.638, time = 0.455\n","3460/125000, loss = -1.490, regloss = -1.63097, valid_loss = -1.681, time = 0.448\n","3470/125000, loss = -1.663, regloss = -1.63233, valid_loss = -1.706, time = 0.449\n","3480/125000, loss = -1.492, regloss = -1.63381, valid_loss = -1.701, time = 0.451\n","3490/125000, loss = -1.423, regloss = -1.63095, valid_loss = -1.711, time = 0.451\n","learning rate: 9.999999747378752e-05\n","3500/125000, loss = -1.620, regloss = -1.63092, valid_loss = -1.696, time = 0.454\n","3510/125000, loss = -1.571, regloss = -1.63414, valid_loss = -1.725, time = 0.448\n","3520/125000, loss = -1.587, regloss = -1.63805, valid_loss = -1.679, time = 0.451\n","3530/125000, loss = -1.493, regloss = -1.63553, valid_loss = -1.716, time = 0.453\n","3540/125000, loss = -1.584, regloss = -1.63377, valid_loss = -1.711, time = 0.454\n","3550/125000, loss = -1.848, regloss = -1.63637, valid_loss = -1.696, time = 0.447\n","3560/125000, loss = -1.703, regloss = -1.63802, valid_loss = -1.674, time = 0.453\n","3570/125000, loss = -1.720, regloss = -1.64365, valid_loss = -1.712, time = 0.448\n","3580/125000, loss = -1.649, regloss = -1.64669, valid_loss = -1.694, time = 0.451\n","3590/125000, loss = -1.435, regloss = -1.64665, valid_loss = -1.716, time = 0.449\n","3600/125000, loss = -1.318, regloss = -1.64227, valid_loss = -1.648, time = 0.454\n","3610/125000, loss = -1.665, regloss = -1.64154, valid_loss = -1.705, time = 0.449\n","3620/125000, loss = -1.756, regloss = -1.64087, valid_loss = -1.702, time = 0.448\n","3630/125000, loss = -1.663, regloss = -1.64283, valid_loss = -1.635, time = 0.450\n","3640/125000, loss = -1.577, regloss = -1.64246, valid_loss = -1.684, time = 0.455\n","3650/125000, loss = -1.727, regloss = -1.64428, valid_loss = -1.708, time = 0.450\n","3660/125000, loss = -1.660, regloss = -1.64736, valid_loss = -1.712, time = 0.450\n","3670/125000, loss = -1.721, regloss = -1.65015, valid_loss = -1.747, time = 0.454\n","3680/125000, loss = -1.758, regloss = -1.65154, valid_loss = -1.737, time = 0.456\n","3690/125000, loss = -1.575, regloss = -1.65365, valid_loss = -1.758, time = 0.454\n","3700/125000, loss = -1.781, regloss = -1.65636, valid_loss = -1.743, time = 0.450\n","3710/125000, loss = -1.641, regloss = -1.65850, valid_loss = -1.743, time = 0.455\n","3720/125000, loss = -1.863, regloss = -1.66582, valid_loss = -1.749, time = 0.452\n","3730/125000, loss = -1.762, regloss = -1.67345, valid_loss = -1.759, time = 0.453\n","3740/125000, loss = -1.642, regloss = -1.67439, valid_loss = -1.757, time = 0.455\n","3750/125000, loss = -1.629, regloss = -1.67439, valid_loss = -1.708, time = 0.459\n","3760/125000, loss = -1.626, regloss = -1.67569, valid_loss = -1.738, time = 0.460\n","3770/125000, loss = -1.634, regloss = -1.67725, valid_loss = -1.754, time = 0.452\n","3780/125000, loss = -1.549, regloss = -1.67645, valid_loss = -1.743, time = 0.460\n","3790/125000, loss = -1.640, regloss = -1.67341, valid_loss = -1.740, time = 0.458\n","3800/125000, loss = -1.730, regloss = -1.67414, valid_loss = -1.759, time = 0.453\n","3810/125000, loss = -1.751, regloss = -1.67522, valid_loss = -1.726, time = 0.452\n","3820/125000, loss = -1.678, regloss = -1.67977, valid_loss = -1.743, time = 0.452\n","3830/125000, loss = -1.764, regloss = -1.68448, valid_loss = -1.739, time = 0.459\n","3840/125000, loss = -1.697, regloss = -1.68548, valid_loss = -1.733, time = 0.456\n","3850/125000, loss = -1.556, regloss = -1.69109, valid_loss = -1.758, time = 0.451\n","3860/125000, loss = -1.505, regloss = -1.69067, valid_loss = -1.762, time = 0.451\n","3870/125000, loss = -1.798, regloss = -1.69529, valid_loss = -1.741, time = 0.455\n","3880/125000, loss = -1.764, regloss = -1.69698, valid_loss = -1.753, time = 0.447\n","3890/125000, loss = -1.644, regloss = -1.69266, valid_loss = -1.739, time = 0.445\n","3900/125000, loss = -1.414, regloss = -1.68939, valid_loss = -1.762, time = 0.459\n","3910/125000, loss = -1.573, regloss = -1.68930, valid_loss = -1.747, time = 0.449\n","3920/125000, loss = -1.922, regloss = -1.69695, valid_loss = -1.732, time = 0.447\n","3930/125000, loss = -1.677, regloss = -1.69599, valid_loss = -1.745, time = 0.450\n","3940/125000, loss = -1.744, regloss = -1.69524, valid_loss = -1.712, time = 0.456\n","3950/125000, loss = -1.519, regloss = -1.68728, valid_loss = -1.705, time = 0.448\n","3960/125000, loss = -1.670, regloss = -1.69099, valid_loss = -1.755, time = 0.448\n","3970/125000, loss = -1.486, regloss = -1.68650, valid_loss = -1.757, time = 0.448\n","3980/125000, loss = -1.697, regloss = -1.69084, valid_loss = -1.737, time = 0.447\n","3990/125000, loss = -1.697, regloss = -1.69307, valid_loss = -1.762, time = 0.450\n","learning rate: 9.999999747378752e-05\n","SAVED MODEL\n","4000/125000, loss = -1.677, regloss = -1.70051, valid_loss = -1.760, time = 0.452\n","4010/125000, loss = -1.597, regloss = -1.70242, valid_loss = -1.741, time = 0.455\n","4020/125000, loss = -1.803, regloss = -1.70299, valid_loss = -1.774, time = 0.451\n","4030/125000, loss = -1.922, regloss = -1.71089, valid_loss = -1.753, time = 0.453\n","4040/125000, loss = -1.653, regloss = -1.71226, valid_loss = -1.774, time = 0.449\n","4050/125000, loss = -1.639, regloss = -1.71302, valid_loss = -1.750, time = 0.447\n","4060/125000, loss = -1.630, regloss = -1.71208, valid_loss = -1.757, time = 0.448\n","4070/125000, loss = -1.610, regloss = -1.70959, valid_loss = -1.779, time = 0.452\n","4080/125000, loss = -1.733, regloss = -1.70630, valid_loss = -1.765, time = 0.449\n","4090/125000, loss = -1.822, regloss = -1.70284, valid_loss = -1.751, time = 0.446\n","4100/125000, loss = -1.717, regloss = -1.70249, valid_loss = -1.737, time = 0.453\n","4110/125000, loss = -1.592, regloss = -1.70333, valid_loss = -1.735, time = 0.446\n","4120/125000, loss = -1.696, regloss = -1.70535, valid_loss = -1.765, time = 0.454\n","4130/125000, loss = -1.738, regloss = -1.70459, valid_loss = -1.769, time = 0.446\n","4140/125000, loss = -2.001, regloss = -1.70839, valid_loss = -1.789, time = 0.446\n","4150/125000, loss = -1.733, regloss = -1.71038, valid_loss = -1.774, time = 0.450\n","4160/125000, loss = -1.587, regloss = -1.70911, valid_loss = -1.771, time = 0.453\n","4170/125000, loss = -1.684, regloss = -1.70790, valid_loss = -1.784, time = 0.446\n","4180/125000, loss = -1.518, regloss = -1.70681, valid_loss = -1.765, time = 0.446\n","4190/125000, loss = -1.702, regloss = -1.71230, valid_loss = -1.761, time = 0.453\n","4200/125000, loss = -1.797, regloss = -1.71775, valid_loss = -1.762, time = 0.447\n","4210/125000, loss = -1.849, regloss = -1.71429, valid_loss = -1.764, time = 0.446\n","4220/125000, loss = -1.643, regloss = -1.71102, valid_loss = -1.782, time = 0.444\n","4230/125000, loss = -1.643, regloss = -1.71288, valid_loss = -1.790, time = 0.449\n","4240/125000, loss = -1.921, regloss = -1.71194, valid_loss = -1.769, time = 0.445\n","4250/125000, loss = -1.580, regloss = -1.70884, valid_loss = -1.763, time = 0.452\n","4260/125000, loss = -1.870, regloss = -1.71111, valid_loss = -1.767, time = 0.448\n","4270/125000, loss = -1.603, regloss = -1.71187, valid_loss = -1.775, time = 0.453\n","4280/125000, loss = -1.570, regloss = -1.71046, valid_loss = -1.783, time = 0.446\n","4290/125000, loss = -1.772, regloss = -1.71274, valid_loss = -1.782, time = 0.457\n","4300/125000, loss = -1.647, regloss = -1.70996, valid_loss = -1.773, time = 0.447\n","4310/125000, loss = -1.650, regloss = -1.71029, valid_loss = -1.776, time = 0.461\n","4320/125000, loss = -1.780, regloss = -1.70989, valid_loss = -1.798, time = 0.445\n","4330/125000, loss = -1.766, regloss = -1.70971, valid_loss = -1.787, time = 0.448\n","4340/125000, loss = -1.629, regloss = -1.71561, valid_loss = -1.761, time = 0.445\n","4350/125000, loss = -1.655, regloss = -1.71445, valid_loss = -1.783, time = 0.445\n","4360/125000, loss = -1.758, regloss = -1.71909, valid_loss = -1.765, time = 0.447\n","4370/125000, loss = -1.812, regloss = -1.72067, valid_loss = -1.778, time = 0.447\n","4380/125000, loss = -1.836, regloss = -1.72197, valid_loss = -1.774, time = 0.446\n","4390/125000, loss = -1.726, regloss = -1.72176, valid_loss = -1.784, time = 0.458\n","4400/125000, loss = -1.708, regloss = -1.72128, valid_loss = -1.770, time = 0.446\n","4410/125000, loss = -1.611, regloss = -1.72614, valid_loss = -1.790, time = 0.452\n","4420/125000, loss = -1.772, regloss = -1.73001, valid_loss = -1.776, time = 0.454\n","4430/125000, loss = -1.725, regloss = -1.73135, valid_loss = -1.756, time = 0.451\n","4440/125000, loss = -1.809, regloss = -1.72969, valid_loss = -1.778, time = 0.449\n","4450/125000, loss = -1.742, regloss = -1.73187, valid_loss = -1.784, time = 0.447\n","4460/125000, loss = -1.960, regloss = -1.73523, valid_loss = -1.779, time = 0.456\n","4470/125000, loss = -1.900, regloss = -1.73965, valid_loss = -1.789, time = 0.454\n","4480/125000, loss = -1.773, regloss = -1.74152, valid_loss = -1.789, time = 0.451\n","4490/125000, loss = -1.564, regloss = -1.74437, valid_loss = -1.795, time = 0.451\n","learning rate: 9.999999747378752e-05\n","4500/125000, loss = -1.679, regloss = -1.73947, valid_loss = -1.784, time = 0.450\n","4510/125000, loss = -1.650, regloss = -1.73819, valid_loss = -1.776, time = 0.449\n","4520/125000, loss = -1.738, regloss = -1.73747, valid_loss = -1.784, time = 0.452\n","4530/125000, loss = -1.673, regloss = -1.73417, valid_loss = -1.787, time = 0.451\n","4540/125000, loss = -1.667, regloss = -1.73135, valid_loss = -1.742, time = 0.453\n","4550/125000, loss = -1.649, regloss = -1.73151, valid_loss = -1.725, time = 0.454\n","4560/125000, loss = -1.619, regloss = -1.71973, valid_loss = -1.757, time = 0.452\n","4570/125000, loss = -1.630, regloss = -1.71546, valid_loss = -1.743, time = 0.462\n","4580/125000, loss = -1.754, regloss = -1.71593, valid_loss = -1.756, time = 0.451\n","4590/125000, loss = -1.701, regloss = -1.71162, valid_loss = -1.756, time = 0.453\n","4600/125000, loss = -1.809, regloss = -1.71080, valid_loss = -1.773, time = 0.452\n","4610/125000, loss = -1.779, regloss = -1.71664, valid_loss = -1.759, time = 0.466\n","4620/125000, loss = -1.396, regloss = -1.71799, valid_loss = -1.769, time = 0.448\n","4630/125000, loss = -1.505, regloss = -1.71816, valid_loss = -1.775, time = 0.454\n","4640/125000, loss = -1.762, regloss = -1.71990, valid_loss = -1.744, time = 0.449\n","4650/125000, loss = -1.926, regloss = -1.72261, valid_loss = -1.792, time = 0.463\n","4660/125000, loss = -1.649, regloss = -1.72241, valid_loss = -1.758, time = 0.453\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-e99b747de6d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;31m#sample_model(args)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-3f86d256bb00>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     43\u001b[0m                         \u001b[0mfeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                         \u001b[0mfeed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_kappa\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkmixtures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                         \u001b[0;34m[\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                         \u001b[0mrunning_average\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_average\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mremember_rate\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mremember_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["!cp -r '/content/model2' \"/content/drive/MyDrive/Colab Notebooks/Handwriting Generation/model2\""],"metadata":{"id":"C3H_SIhIU57v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"uwbsrNbwOaHW"},"execution_count":null,"outputs":[]}]}